# æ¨¡å‹ (Models)

[å¤§è¯­è¨€æ¨¡å‹ (LLMs)](https://en.wikipedia.org/wiki/Large_language_model) æ˜¯å¼ºå¤§çš„ AI å·¥å…·ï¼Œèƒ½å¤Ÿåƒäººç±»ä¸€æ ·ç†è§£å’Œç”Ÿæˆæ–‡æœ¬ã€‚å®ƒä»¬åŠŸèƒ½å¤šæ ·ï¼Œè¶³ä»¥å®Œæˆå†…å®¹åˆ›ä½œã€è¯­è¨€ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦å’Œé—®ç­”ç­‰ä»»åŠ¡ï¼Œè€Œæ— éœ€é’ˆå¯¹æ¯ä¸ªä»»åŠ¡è¿›è¡Œä¸“é—¨è®­ç»ƒã€‚

é™¤äº†æ–‡æœ¬ç”Ÿæˆä¹‹å¤–ï¼Œè®¸å¤šæ¨¡å‹è¿˜æ”¯æŒä»¥ä¸‹åŠŸèƒ½ï¼š

- ğŸ”¨ [å·¥å…·è°ƒç”¨ (Tool calling)](#tool-calling) - è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼ˆå¦‚æ•°æ®åº“æŸ¥è¯¢æˆ– API è°ƒç”¨ï¼‰å¹¶åœ¨å“åº”ä¸­ä½¿ç”¨ç»“æœã€‚

  - **è¯¦ç»†è¯´æ˜ï¼š** å·¥å…·è°ƒç”¨è®©æ¨¡å‹èƒ½å¤Ÿçªç ´çº¯æ–‡æœ¬ç”Ÿæˆçš„é™åˆ¶ï¼Œä¸å¤–éƒ¨ç³»ç»Ÿäº¤äº’ã€‚ä¾‹å¦‚ï¼Œå½“ä½ é—®"ä»Šå¤©åŒ—äº¬çš„å¤©æ°”å¦‚ä½•ï¼Ÿ"æ—¶ï¼Œæ¨¡å‹å¯ä»¥è°ƒç”¨å¤©æ°” API è·å–å®æ—¶æ•°æ®ï¼Œè€Œä¸æ˜¯åŸºäºè®­ç»ƒæ•°æ®çŒœæµ‹ã€‚è¿™å¤§å¤§æ‰©å±•äº†æ¨¡å‹çš„å®ç”¨æ€§ã€‚

- ğŸ”· [ç»“æ„åŒ–è¾“å‡º (Structured output)](#structured-outputs) - æ¨¡å‹çš„å“åº”è¢«çº¦æŸä¸ºéµå¾ªå®šä¹‰çš„æ ¼å¼ã€‚

  - **è¯¦ç»†è¯´æ˜ï¼š** ç»“æ„åŒ–è¾“å‡ºç¡®ä¿æ¨¡å‹è¿”å›çš„æ•°æ®ç¬¦åˆç‰¹å®šçš„æ ¼å¼è§„èŒƒï¼ˆå¦‚ JSON Schemaã€Pydantic æ¨¡å‹ç­‰ï¼‰ã€‚è¿™å¯¹äºéœ€è¦å°†æ¨¡å‹è¾“å‡ºç›´æ¥é›†æˆåˆ°åº”ç”¨ç¨‹åºä¸­çš„åœºæ™¯è‡³å…³é‡è¦ï¼Œå¯ä»¥é¿å…è§£æé”™è¯¯å’Œæ•°æ®éªŒè¯é—®é¢˜ã€‚

- ğŸ–¼ï¸ [å¤šæ¨¡æ€ (Multimodality)](#multimodal) - å¤„ç†å’Œè¿”å›æ–‡æœ¬ä»¥å¤–çš„æ•°æ®ï¼Œå¦‚å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ã€‚

  - **è¯¦ç»†è¯´æ˜ï¼š** å¤šæ¨¡æ€æ¨¡å‹å¯ä»¥ç†è§£å’Œç”Ÿæˆå¤šç§ç±»å‹çš„æ•°æ®ã€‚ä¾‹å¦‚ï¼Œå®ƒä»¬å¯ä»¥åˆ†æå›¾ç‰‡å†…å®¹ã€ç”Ÿæˆå›¾åƒã€è½¬å½•è¯­éŸ³æˆ–ç†è§£è§†é¢‘åœºæ™¯ã€‚è¿™ä½¿å¾— AI åº”ç”¨èƒ½å¤Ÿå¤„ç†æ›´ä¸°å¯Œçš„ç°å®ä¸–ç•Œæ•°æ®ã€‚

- ğŸ§  [æ¨ç† (Reasoning)](#reasoning) - æ¨¡å‹æ‰§è¡Œå¤šæ­¥æ¨ç†ä»¥å¾—å‡ºç»“è®ºã€‚
  - **è¯¦ç»†è¯´æ˜ï¼š** æ¨ç†èƒ½åŠ›ä½¿æ¨¡å‹èƒ½å¤Ÿåƒäººç±»ä¸€æ ·è¿›è¡Œé€»è¾‘æ€è€ƒã€‚æ¨¡å‹ä¼šå°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªæ­¥éª¤ï¼Œé€æ­¥æ¨å¯¼ï¼Œè€Œä¸æ˜¯ç›´æ¥ç»™å‡ºç­”æ¡ˆã€‚è¿™å¯¹äºè§£å†³æ•°å­¦é—®é¢˜ã€é€»è¾‘è°œé¢˜æˆ–éœ€è¦æ·±åº¦åˆ†æçš„ä»»åŠ¡ç‰¹åˆ«æœ‰ç”¨ã€‚

Models æ˜¯ [æ™ºèƒ½ä½“ (agents)](/oss/python/langchain/agents) çš„æ¨ç†å¼•æ“ã€‚å®ƒä»¬é©±åŠ¨æ™ºèƒ½ä½“çš„å†³ç­–è¿‡ç¨‹ï¼Œå†³å®šè°ƒç”¨å“ªäº›å·¥å…·ã€å¦‚ä½•è§£é‡Šç»“æœä»¥åŠä½•æ—¶æä¾›æœ€ç»ˆç­”æ¡ˆã€‚

LangChain æä¾›äº†ç»Ÿä¸€çš„æŠ½è±¡å±‚ï¼Œè¿™æ„å‘³ç€æ‚¨å¯ä»¥ä½¿ç”¨ç›¸åŒçš„ä»£ç ä¸ä¸åŒæä¾›å•†çš„æ¨¡å‹äº¤äº’ã€‚è¿™ç§è®¾è®¡å¸¦æ¥äº†å·¨å¤§çš„çµæ´»æ€§â€”â€”æ‚¨å¯ä»¥åœ¨å¼€å‘æ—¶ä½¿ç”¨å»‰ä»·çš„æ¨¡å‹ï¼Œåœ¨ç”Ÿäº§ç¯å¢ƒåˆ‡æ¢åˆ°æ›´å¼ºå¤§çš„æ¨¡å‹ï¼Œè€Œæ— éœ€é‡å†™ä»£ç ã€‚

## åŸºæœ¬ç”¨æ³• (Basic usage)

æ¨¡å‹å¯ä»¥é€šè¿‡ä¸¤ç§æ–¹å¼ä½¿ç”¨ï¼š

init_chat_model å’Œ Model Class

### åˆå§‹åŒ–æ¨¡å‹ (Initialize a model)

åœ¨ LangChain ä¸­å¼€å§‹ä½¿ç”¨ç‹¬ç«‹æ¨¡å‹çš„æœ€ç®€å•æ–¹æ³•æ˜¯ä½¿ç”¨ [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model) ä»æ‚¨é€‰æ‹©çš„[æä¾›å•†](/oss/python/integrations/providers/overview)åˆå§‹åŒ–ä¸€ä¸ªæ¨¡å‹ï¼ˆç¤ºä¾‹å¦‚ä¸‹ï¼‰ï¼š

**ä½¿ç”¨ init_chat_model:**

```python
import os
from langchain.chat_models import init_chat_model
from dotenv import load_dotenv

load_dotenv()

model = init_chat_model(
    model="qwen3-max",
    model_provider="openai",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    api_key=os.getenv("DASHSCOPE_API_KEY"),
)

response = model.invoke("ä½ å¥½")
print(response.content)
```

**ä½¿ç”¨ Model Class:**

```python
from langchain_openai import ChatOpenAI
import os
from dotenv import load_dotenv

load_dotenv()

chatLLM = ChatOpenAI(
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    model="qwen-plus",
)

response = chatLLM.invoke(â€œä½ å¥½â€)
print(response.content)
```

ä¸¤ç§æ–¹å¼æœ¬è´¨ä¸Šæ˜¯ç›¸åŒçš„ - init_chat_model æœ€ç»ˆä¹Ÿä¼šå®ä¾‹åŒ–å¯¹åº”çš„ Model Classã€‚é€‰æ‹©å“ªç§æ–¹å¼ä¸»è¦å–å†³äº:

- çµæ´»æ€§éœ€æ±‚: å¦‚æœéœ€è¦åŠ¨æ€åˆ‡æ¢æ¨¡å‹ â†’ init_chat_model

- æ˜ç¡®æ€§éœ€æ±‚: å¦‚æœæ¨¡å‹ç¡®å®šä¸”éœ€è¦ç±»å‹å®‰å…¨ â†’ Model Class

- ä½ çš„åœºæ™¯: å¯¹äº DashScope è¿™ç§å…¼å®¹ OpenAI API çš„æœåŠ¡,ä¸¤ç§æ–¹å¼éƒ½å¯ä»¥,ä½† init_chat_model æ›´ç®€æ´

### æ ¸å¿ƒæ–¹æ³• (Key methods)

#### ğŸ“¤ [Invokeï¼ˆè°ƒç”¨)](#invoke)

æ¨¡å‹æ¥å—æ¶ˆæ¯ä½œä¸ºè¾“å…¥ï¼Œåœ¨ç”Ÿæˆå®Œæ•´å“åº”åè¾“å‡ºæ¶ˆæ¯ã€‚

**è¯¦ç»†è¯´æ˜ï¼š** è¿™æ˜¯æœ€åŸºæœ¬çš„åŒæ­¥è°ƒç”¨æ–¹å¼ï¼Œé€‚åˆéœ€è¦å®Œæ•´å“åº”åå†ç»§ç»­æ‰§è¡Œçš„åœºæ™¯ã€‚

#### ğŸ“¡ [Streamï¼ˆæµå¼ä¼ è¾“ï¼‰](#stream)

è°ƒç”¨æ¨¡å‹ï¼Œä½†åœ¨ç”Ÿæˆè¾“å‡ºæ—¶å®æ—¶æµå¼ä¼ è¾“ã€‚

**è¯¦ç»†è¯´æ˜ï¼š** æµå¼ä¼ è¾“æ˜¾è‘—æ”¹å–„ç”¨æˆ·ä½“éªŒï¼Œç‰¹åˆ«æ˜¯å¯¹äºé•¿æ–‡æœ¬ç”Ÿæˆã€‚ç”¨æˆ·å¯ä»¥ç«‹å³çœ‹åˆ°è¾“å‡ºå¼€å§‹å‡ºç°ï¼Œè€Œä¸æ˜¯ç­‰å¾…æ•´ä¸ªå“åº”å®Œæˆã€‚

#### ğŸ“Š [Batchï¼ˆæ‰¹å¤„ç†ï¼‰](#batch)

æ‰¹é‡å‘é€å¤šä¸ªè¯·æ±‚åˆ°æ¨¡å‹ä»¥å®ç°æ›´é«˜æ•ˆçš„å¤„ç†ã€‚

**è¯¦ç»†è¯´æ˜ï¼š** æ‰¹å¤„ç†å¯ä»¥å¹¶è¡Œå¤„ç†å¤šä¸ªç‹¬ç«‹è¯·æ±‚ï¼Œå¤§å¤§æé«˜ååé‡å’Œæ•ˆç‡ï¼Œç‰¹åˆ«é€‚åˆå¤„ç†å¤§é‡æ•°æ®çš„åœºæ™¯ã€‚

## å‚æ•° (Parameters)

èŠå¤©æ¨¡å‹æ¥å—å¯ç”¨äºé…ç½®å…¶è¡Œä¸ºçš„å‚æ•°ã€‚æ”¯æŒçš„å®Œæ•´å‚æ•°é›†å› æ¨¡å‹å’Œæä¾›å•†è€Œå¼‚ï¼Œä½†æ ‡å‡†å‚æ•°åŒ…æ‹¬ï¼š

**è¯¦ç»†è¯´æ˜ï¼š** å‚æ•°é…ç½®æ˜¯è°ƒä¼˜æ¨¡å‹è¡Œä¸ºçš„å…³é”®ã€‚é€šè¿‡è°ƒæ•´è¿™äº›å‚æ•°ï¼Œæ‚¨å¯ä»¥åœ¨åˆ›é€ æ€§ã€ç¡®å®šæ€§ã€æˆæœ¬å’Œæ€§èƒ½ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ã€‚ä¾‹å¦‚ï¼Œè¾ƒé«˜çš„ `temperature` ä¼šäº§ç”Ÿæ›´æœ‰åˆ›æ„ä½†å¯èƒ½ä¸å¤ªä¸€è‡´çš„è¾“å‡ºï¼Œè€Œè¾ƒä½çš„å€¼åˆ™äº§ç”Ÿæ›´å¯é¢„æµ‹çš„ç»“æœã€‚

- **model** (string, å¿…éœ€)

  è¦ä¸æä¾›å•†ä¸€èµ·ä½¿ç”¨çš„ç‰¹å®šæ¨¡å‹çš„åç§°æˆ–æ ‡è¯†ç¬¦ã€‚

  **è¯¦ç»†è¯´æ˜ï¼š** ä¾‹å¦‚ "gpt-4"ã€"claude-3-5-sonnet" ç­‰ã€‚ä¸åŒçš„æ¨¡å‹æœ‰ä¸åŒçš„èƒ½åŠ›ã€é€Ÿåº¦å’Œæˆæœ¬ç‰¹å¾ã€‚

- **api_key** (string)

  ä¸æ¨¡å‹æä¾›å•†è¿›è¡Œèº«ä»½éªŒè¯æ‰€éœ€çš„å¯†é’¥ã€‚é€šå¸¸åœ¨æ³¨å†Œè®¿é—®æ¨¡å‹æ—¶é¢å‘ã€‚é€šå¸¸é€šè¿‡è®¾ç½®ç¯å¢ƒå˜é‡æ¥è®¿é—®ï¼ˆç¯å¢ƒå˜é‡ï¼šä¸€ä¸ªå…¶å€¼åœ¨ç¨‹åºå¤–éƒ¨è®¾ç½®çš„å˜é‡ï¼Œé€šå¸¸é€šè¿‡æ“ä½œç³»ç»Ÿæˆ–å¾®æœåŠ¡çš„å†…ç½®åŠŸèƒ½ï¼‰ã€‚

  **è¯¦ç»†è¯´æ˜ï¼š** API å¯†é’¥ç”¨äºèº«ä»½éªŒè¯å’Œè®¡è´¹ã€‚æ°¸è¿œä¸è¦åœ¨ä»£ç ä¸­ç¡¬ç¼–ç  API å¯†é’¥ï¼Œåº”è¯¥ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼ˆå¦‚ `OPENAI_API_KEY`ï¼‰æˆ–å¯†é’¥ç®¡ç†æœåŠ¡ã€‚è¿™æ ·å¯ä»¥ä¿æŠ¤å¯†é’¥ä¸è¢«æ„å¤–æ³„éœ²åˆ°ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿä¸­ã€‚

- **temperature** (number)

  æ§åˆ¶æ¨¡å‹è¾“å‡ºçš„éšæœºæ€§ã€‚è¾ƒé«˜çš„æ•°å­—ä½¿å“åº”æ›´å…·åˆ›é€ æ€§ï¼›è¾ƒä½çš„æ•°å­—ä½¿å“åº”æ›´å…·ç¡®å®šæ€§ã€‚

  **è¯¦ç»†è¯´æ˜ï¼š** `temperature` é€šå¸¸åœ¨ 0 åˆ° 2 ä¹‹é—´ï¼ˆå…·ä½“èŒƒå›´å–å†³äºæä¾›å•†ï¼‰ï¼š

  - **0-0.3**: é«˜åº¦ç¡®å®šæ€§ï¼Œé€‚åˆäº‹å®æ€§ä»»åŠ¡ã€ä»£ç ç”Ÿæˆã€æ•°æ®æå–
  - **0.7-1.0**: å¹³è¡¡åˆ›é€ æ€§å’Œä¸€è‡´æ€§ï¼Œé€‚åˆé€šç”¨å¯¹è¯
  - **1.0+**: é«˜åº¦åˆ›é€ æ€§ï¼Œé€‚åˆåˆ›æ„å†™ä½œã€å¤´è„‘é£æš´

- **timeout** (number)

  åœ¨å–æ¶ˆè¯·æ±‚ä¹‹å‰ç­‰å¾…æ¨¡å‹å“åº”çš„æœ€é•¿æ—¶é—´ï¼ˆä»¥ç§’ä¸ºå•ä½ï¼‰ã€‚

  **è¯¦ç»†è¯´æ˜ï¼š** è¶…æ—¶è®¾ç½®å¯ä»¥é˜²æ­¢åº”ç”¨ç¨‹åºå› æ¨¡å‹å“åº”ç¼“æ…¢è€ŒæŒ‚èµ·ã€‚å¯¹äºç”Ÿäº§ç¯å¢ƒï¼Œå»ºè®®è®¾ç½®åˆç†çš„è¶…æ—¶å€¼ï¼ˆå¦‚ 30-60 ç§’ï¼‰ã€‚å¦‚æœç»å¸¸è¶…æ—¶ï¼Œå¯èƒ½éœ€è¦è€ƒè™‘ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹æˆ–ä¼˜åŒ–æç¤ºã€‚

- **max_tokens** (number)

  é™åˆ¶å“åº”ä¸­çš„ token æ€»æ•°ï¼Œæœ‰æ•ˆæ§åˆ¶è¾“å‡ºçš„é•¿åº¦ã€‚ï¼ˆTokenï¼šæ¨¡å‹è¯»å–å’Œç”Ÿæˆçš„åŸºæœ¬å•ä½ã€‚æä¾›å•†å¯èƒ½æœ‰ä¸åŒçš„å®šä¹‰ï¼Œä½†é€šå¸¸å®ƒä»¬å¯ä»¥è¡¨ç¤ºä¸€ä¸ªå®Œæ•´çš„å•è¯æˆ–å•è¯çš„ä¸€éƒ¨åˆ†ã€‚ï¼‰

  **è¯¦ç»†è¯´æ˜ï¼š** Token æ˜¯æ¨¡å‹å¤„ç†æ–‡æœ¬çš„åŸºæœ¬å•ä½ã€‚å¯¹äºè‹±æ–‡ï¼Œ1 ä¸ª token çº¦ç­‰äº 0.75 ä¸ªå•è¯ï¼›å¯¹äºä¸­æ–‡ï¼Œ1 ä¸ªæ±‰å­—é€šå¸¸æ˜¯ 1-2 ä¸ª tokenã€‚è®¾ç½® `max_tokens` å¯ä»¥ï¼š

  - æ§åˆ¶æˆæœ¬ï¼ˆå¤§å¤šæ•°æä¾›å•†æŒ‰ token è®¡è´¹ï¼‰
  - é˜²æ­¢è¿‡é•¿çš„å“åº”
  - ç¡®ä¿å“åº”é€‚åˆæ‚¨çš„ UI é™åˆ¶

  æ³¨æ„ï¼šè¾“å…¥å’Œè¾“å‡º token éƒ½è®¡å…¥æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ã€‚

- **max_retries** (number)

  å¦‚æœç”±äºç½‘ç»œè¶…æ—¶æˆ–é€Ÿç‡é™åˆ¶ç­‰é—®é¢˜å¯¼è‡´è¯·æ±‚å¤±è´¥ï¼Œç³»ç»Ÿå°†å°è¯•é‡æ–°å‘é€è¯·æ±‚çš„æœ€å¤§æ¬¡æ•°ã€‚

  **è¯¦ç»†è¯´æ˜ï¼š** è‡ªåŠ¨é‡è¯•æœºåˆ¶å¯ä»¥æé«˜åº”ç”¨ç¨‹åºçš„å¯é æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ç¬æ—¶ç½‘ç»œé—®é¢˜æˆ– API é€Ÿç‡é™åˆ¶æ—¶ã€‚LangChain ä¼šä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥ï¼Œåœ¨é‡è¯•ä¹‹é—´é€æ¸å¢åŠ ç­‰å¾…æ—¶é—´ã€‚

ä½¿ç”¨ [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model),å°†è¿™äº›å‚æ•°ä½œä¸ºå†…è” `**kwargs`(ä»»æ„å…³é”®å­—å‚æ•°,æ›´å¤šä¿¡æ¯è§ [Python args kwargs](https://www.w3schools.com/python/python_args_kwargs.asp))ä¼ é€’:

```python
from langchain.chat_models import init_chat_model
import os
from dotenv import load_dotenv

load_dotenv()

model = init_chat_model(
    model="qwen-plus",
    model_provider="openai",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    # Kwargs passed to the model:
    temperature=0.7,
    timeout=30,
    max_tokens=1000,
)
```

> **â„¹ï¸ æç¤º**: æ¯ä¸ªèŠå¤©æ¨¡å‹é›†æˆå¯èƒ½æœ‰é¢å¤–çš„å‚æ•°ç”¨äºæ§åˆ¶æä¾›å•†ç‰¹å®šçš„åŠŸèƒ½ã€‚ä¾‹å¦‚,[`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain_openai/ChatOpenAI/) æœ‰ `use_responses_api` æ¥æŒ‡å®šæ˜¯å¦ä½¿ç”¨ OpenAI Responses æˆ– Completions APIã€‚è¦æŸ¥æ‰¾ç»™å®šèŠå¤©æ¨¡å‹æ”¯æŒçš„æ‰€æœ‰å‚æ•°,è¯·è®¿é—®[èŠå¤©æ¨¡å‹é›†æˆ](/oss/python/integrations/chat)é¡µé¢ã€‚

---

## è°ƒç”¨ (Invocation)

å¿…é¡»è°ƒç”¨èŠå¤©æ¨¡å‹æ‰èƒ½ç”Ÿæˆè¾“å‡ºã€‚æœ‰ä¸‰ç§ä¸»è¦çš„è°ƒç”¨æ–¹æ³•ï¼Œæ¯ç§éƒ½é€‚åˆä¸åŒçš„ä½¿ç”¨åœºæ™¯ã€‚

**è¯¦ç»†è¯´æ˜ï¼š** é€‰æ‹©åˆé€‚çš„è°ƒç”¨æ–¹æ³•å¯¹åº”ç”¨ç¨‹åºçš„æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒè‡³å…³é‡è¦ï¼š

- **Invoke**: é€‚åˆéœ€è¦å®Œæ•´å“åº”çš„ç®€å•åœºæ™¯
- **Stream**: é€‚åˆäº¤äº’å¼åº”ç”¨ï¼Œæä¾›å®æ—¶åé¦ˆ
- **Batch**: é€‚åˆå¤„ç†å¤§é‡ç‹¬ç«‹è¯·æ±‚çš„åœºæ™¯

### Invokeï¼ˆè°ƒç”¨ï¼‰

è°ƒç”¨æ¨¡å‹æœ€ç›´æ¥çš„æ–¹æ³•æ˜¯ä½¿ç”¨ [`invoke()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.invoke) é…åˆå•ä¸ªæ¶ˆæ¯æˆ–æ¶ˆæ¯åˆ—è¡¨ã€‚

**è¯¦ç»†è¯´æ˜ï¼š** `invoke()` æ˜¯ä¸€ä¸ªé˜»å¡è°ƒç”¨ï¼Œæ„å‘³ç€æ‚¨çš„ç¨‹åºä¼šç­‰å¾…æ¨¡å‹å®Œæˆæ•´ä¸ªå“åº”åå†ç»§ç»­ã€‚è¿™å¯¹äºæ‰¹å¤„ç†è„šæœ¬æˆ–ä¸éœ€è¦å®æ—¶åé¦ˆçš„åœºæ™¯å¾ˆç†æƒ³ã€‚

```python
response = model.invoke("ä¸ºä»€ä¹ˆé¹¦é¹‰æœ‰äº”é¢œå…­è‰²çš„ç¾½æ¯›ï¼Ÿ")
print(response.content)
```

å¯ä»¥å‘æ¨¡å‹æä¾›æ¶ˆæ¯åˆ—è¡¨æ¥è¡¨ç¤ºå¯¹è¯å†å²ã€‚æ¯æ¡æ¶ˆæ¯éƒ½æœ‰ä¸€ä¸ªè§’è‰²ï¼Œæ¨¡å‹ä½¿ç”¨è¯¥è§’è‰²æ¥æŒ‡ç¤ºåœ¨å¯¹è¯ä¸­è°å‘é€äº†è¯¥æ¶ˆæ¯ã€‚æœ‰å…³è§’è‰²ã€ç±»å‹å’Œå†…å®¹çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[æ¶ˆæ¯æŒ‡å—](/oss/python/langchain/messages)ã€‚

**è¯¦ç»†è¯´æ˜ï¼š** æ¶ˆæ¯å†å²å¯¹äºæ„å»ºå¯¹è¯å¼åº”ç”¨è‡³å…³é‡è¦ã€‚é€šè¿‡æä¾›ä¸Šä¸‹æ–‡ï¼Œæ¨¡å‹å¯ä»¥ï¼š

- è®°ä½ä¹‹å‰çš„å¯¹è¯å†…å®¹
- ç†è§£ä»£è¯å¼•ç”¨ï¼ˆå¦‚"å®ƒ"ã€"é‚£ä¸ª"ï¼‰
- ä¿æŒå¯¹è¯çš„è¿è´¯æ€§å’Œä¸€è‡´æ€§
- éµå¾ªå¤šè½®å¯¹è¯ä¸­å»ºç«‹çš„æŒ‡ä»¤

æ¶ˆæ¯è§’è‰²åŒ…æ‹¬ï¼š

- **system**: è®¾ç½®æ¨¡å‹çš„è¡Œä¸ºå’Œè§’è‰²ï¼ˆå¦‚"ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"ï¼‰
- **user**: ç”¨æˆ·çš„è¾“å…¥æˆ–é—®é¢˜
- **assistant**: æ¨¡å‹ä¹‹å‰çš„å“åº”
- **tool**: å·¥å…·è°ƒç”¨çš„ç»“æœ

**å­—å…¸æ ¼å¼:**

```python
conversation = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ï¼Œè´Ÿè´£å°†ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡ã€‚"},
    {"role": "user", "content": "ç¿»è¯‘ï¼šæˆ‘å–œæ¬¢ç¼–ç¨‹ã€‚"},
    {"role": "assistant", "content": "I love programming."},
    {"role": "user", "content": "ç¿»è¯‘ï¼šæˆ‘å–œæ¬¢æ„å»ºåº”ç”¨ç¨‹åºã€‚"}
]

response = model.invoke(conversation)
print(response.content)  # I love building applications.
```

**æ¶ˆæ¯å¯¹è±¡æ ¼å¼:**

```python
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

conversation = [
    SystemMessage("ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ï¼Œè´Ÿè´£å°†ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡ã€‚"),
    HumanMessage("ç¿»è¯‘ï¼šæˆ‘å–œæ¬¢ç¼–ç¨‹ã€‚"),
    AIMessage("I love programming."),
    HumanMessage("ç¿»è¯‘ï¼šæˆ‘å–œæ¬¢æ„å»ºåº”ç”¨ç¨‹åºã€‚")
]

response = model.invoke(conversation)
print(response.content)  # I love building applications.
```

### Streamï¼ˆæµå¼ä¼ è¾“ï¼‰

å¤§å¤šæ•°æ¨¡å‹å¯ä»¥åœ¨ç”Ÿæˆè¾“å‡ºå†…å®¹æ—¶è¿›è¡Œæµå¼ä¼ è¾“ã€‚é€šè¿‡é€æ­¥æ˜¾ç¤ºè¾“å‡ºï¼Œæµå¼ä¼ è¾“æ˜¾è‘—æ”¹å–„äº†ç”¨æˆ·ä½“éªŒï¼Œç‰¹åˆ«æ˜¯å¯¹äºè¾ƒé•¿çš„å“åº”ã€‚

**è¯¦ç»†è¯´æ˜ï¼š** æµå¼ä¼ è¾“çš„ä¼˜åŠ¿ï¼š

1. **æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ**: ç”¨æˆ·ç«‹å³çœ‹åˆ°å“åº”å¼€å§‹å‡ºç°ï¼Œè€Œä¸æ˜¯ç›¯ç€åŠ è½½åŠ¨ç”»ç­‰å¾…
2. **æ„ŸçŸ¥æ€§èƒ½æå‡**: å³ä½¿æ€»æ—¶é—´ç›¸åŒï¼Œæµå¼æ˜¾ç¤ºè®©åº”ç”¨æ„Ÿè§‰æ›´å¿«
3. **æ—©æœŸé”™è¯¯æ£€æµ‹**: å¯ä»¥åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ£€æµ‹é—®é¢˜ï¼Œè€Œä¸æ˜¯ç­‰åˆ°æœ€å
4. **é€‚åˆé•¿æ–‡æœ¬**: å¯¹äºç”Ÿæˆé•¿æ–‡æ¡£æˆ–ä»£ç ï¼Œæµå¼ä¼ è¾“æ˜¯å¿…ä¸å¯å°‘çš„

å®é™…åº”ç”¨åœºæ™¯ï¼š

- èŠå¤©æœºå™¨äººç•Œé¢ï¼ˆç±»ä¼¼ ChatGPT çš„é€å­—æ˜¾ç¤ºï¼‰
- ä»£ç ç”Ÿæˆå·¥å…·
- é•¿æ–‡æœ¬æ‘˜è¦æˆ–ç¿»è¯‘
- å®æ—¶å†…å®¹åˆ›ä½œåŠ©æ‰‹

è°ƒç”¨ [`stream()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.stream) è¿”å›ä¸€ä¸ª<Tooltip tip="ä¸€ä¸ªå¯¹è±¡ï¼ŒæŒ‰é¡ºåºé€æ­¥æä¾›å¯¹é›†åˆä¸­æ¯ä¸ªé¡¹çš„è®¿é—®ã€‚">è¿­ä»£å™¨</Tooltip>ï¼Œå®ƒä¼šåœ¨ç”Ÿæˆè¾“å‡ºå—æ—¶äº§å‡ºå®ƒä»¬ã€‚æ‚¨å¯ä»¥ä½¿ç”¨å¾ªç¯å®æ—¶å¤„ç†æ¯ä¸ªå—ï¼š

**è¯¦ç»†è¯´æ˜ï¼š** è¿­ä»£å™¨æ¨¡å¼ä½¿å¾—æµå¼ä¼ è¾“åœ¨ Python ä¸­éå¸¸è‡ªç„¶ã€‚æ¯æ¬¡å¾ªç¯è¿­ä»£éƒ½ä¼šç­‰å¾…ä¸‹ä¸€ä¸ªå—åˆ°è¾¾ï¼Œç„¶åç«‹å³å¤„ç†å®ƒã€‚è¿™ç§æ–¹å¼å†…å­˜æ•ˆç‡é«˜ï¼Œå› ä¸ºä¸éœ€è¦ä¸€æ¬¡æ€§å­˜å‚¨æ•´ä¸ªå“åº”ã€‚

**åŸºæœ¬æ–‡æœ¬æµå¼ä¼ è¾“:**

```python
for chunk in model.stream("ä¸ºä»€ä¹ˆé¹¦é¹‰æœ‰äº”é¢œå…­è‰²çš„ç¾½æ¯›ï¼Ÿ"):
    print(chunk.content, end="", flush=True)
```

**æµå¼ä¼ è¾“å·¥å…·è°ƒç”¨ã€æ¨ç†å’Œå…¶ä»–å†…å®¹:**

```python
for chunk in model.stream("å¤©ç©ºæ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿ"):
    # è·å–å®Œæ•´å†…å®¹
    print(chunk.content, end="", flush=True)
```

As opposed to [`invoke()`](#invoke), which returns a single [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) after the model has finished generating its full response, `stream()` returns multiple [`AIMessageChunk`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessageChunk) objects, each containing a portion of the output text. Importantly, each chunk in a stream is designed to be gathered into a full message via summation:

```python
full = None  # None | AIMessageChunk
for chunk in model.stream("å¤©ç©ºæ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿ"):
    full = chunk if full is None else full + chunk
    print(full.content)

# å¤©
# å¤©ç©º
# å¤©ç©ºæ˜¯
# å¤©ç©ºæ˜¯è“
# å¤©ç©ºæ˜¯è“è‰²
# å¤©ç©ºæ˜¯è“è‰²çš„
# ...

print(full.content)
# å¤©ç©ºæ˜¯è“è‰²çš„...
```

The resulting message can be treated the same as a message that was generated with [`invoke()`](#invoke) - for example, it can be aggregated into a message history and passed back to the model as conversational context.

> **âš ï¸ è­¦å‘Š**: æµå¼ä¼ è¾“ä»…åœ¨ç¨‹åºä¸­çš„æ‰€æœ‰æ­¥éª¤éƒ½çŸ¥é“å¦‚ä½•å¤„ç†å—æµæ—¶æ‰æœ‰æ•ˆã€‚ä¾‹å¦‚,ä¸å…·å¤‡æµå¼ä¼ è¾“èƒ½åŠ›çš„åº”ç”¨ç¨‹åºæ˜¯éœ€è¦åœ¨å¤„ç†ä¹‹å‰å°†æ•´ä¸ªè¾“å‡ºå­˜å‚¨åœ¨å†…å­˜ä¸­çš„åº”ç”¨ç¨‹åºã€‚

#### é«˜çº§æµå¼ä¼ è¾“ä¸»é¢˜

##### "è‡ªåŠ¨æµå¼ä¼ è¾“"èŠå¤©æ¨¡å‹

LangChain é€šè¿‡åœ¨æŸäº›æƒ…å†µä¸‹è‡ªåŠ¨å¯ç”¨æµå¼ä¼ è¾“æ¨¡å¼æ¥ç®€åŒ–ä»èŠå¤©æ¨¡å‹è¿›è¡Œæµå¼ä¼ è¾“,å³ä½¿æ‚¨æ²¡æœ‰æ˜¾å¼è°ƒç”¨æµå¼ä¼ è¾“æ–¹æ³•ã€‚å½“æ‚¨ä½¿ç”¨éæµå¼ä¼ è¾“çš„ invoke æ–¹æ³•ä½†ä»ç„¶å¸Œæœ›æµå¼ä¼ è¾“æ•´ä¸ªåº”ç”¨ç¨‹åº(åŒ…æ‹¬æ¥è‡ªèŠå¤©æ¨¡å‹çš„ä¸­é—´ç»“æœ)æ—¶,è¿™ç‰¹åˆ«æœ‰ç”¨ã€‚

ä¾‹å¦‚,åœ¨ [LangGraph agents](/oss/python/langchain/agents) ä¸­,æ‚¨å¯ä»¥åœ¨èŠ‚ç‚¹å†…è°ƒç”¨ `model.invoke()`,ä½†å¦‚æœåœ¨æµå¼ä¼ è¾“æ¨¡å¼ä¸‹è¿è¡Œ,LangChain å°†è‡ªåŠ¨å§”æ‰˜ç»™æµå¼ä¼ è¾“ã€‚

**å·¥ä½œåŸç†**

å½“æ‚¨ `invoke()` ä¸€ä¸ªèŠå¤©æ¨¡å‹æ—¶,å¦‚æœ LangChain æ£€æµ‹åˆ°æ‚¨æ­£åœ¨å°è¯•æµå¼ä¼ è¾“æ•´ä¸ªåº”ç”¨ç¨‹åº,å®ƒå°†è‡ªåŠ¨åˆ‡æ¢åˆ°å†…éƒ¨æµå¼ä¼ è¾“æ¨¡å¼ã€‚å°±ä½¿ç”¨ invoke çš„ä»£ç è€Œè¨€,è°ƒç”¨çš„ç»“æœå°†æ˜¯ç›¸åŒçš„;ç„¶è€Œ,åœ¨èŠå¤©æ¨¡å‹è¢«æµå¼ä¼ è¾“æ—¶,LangChain å°†è´Ÿè´£åœ¨ LangChain çš„å›è°ƒç³»ç»Ÿä¸­è°ƒç”¨ [`on_llm_new_token`](https://reference.langchain.com/python/langchain_core/callbacks/#langchain_core.callbacks.base.AsyncCallbackHandler.on_llm_new_token) äº‹ä»¶ã€‚

å›è°ƒäº‹ä»¶å…è®¸ LangGraph `stream()` å’Œ [`astream_events()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.astream_events) å®æ—¶æ˜¾ç¤ºèŠå¤©æ¨¡å‹çš„è¾“å‡ºã€‚

##### æµå¼ä¼ è¾“äº‹ä»¶

LangChain èŠå¤©æ¨¡å‹è¿˜å¯ä»¥ä½¿ç”¨ [`astream_events()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.astream_events) æµå¼ä¼ è¾“è¯­ä¹‰äº‹ä»¶ã€‚

è¿™ç®€åŒ–äº†åŸºäºäº‹ä»¶ç±»å‹å’Œå…¶ä»–å…ƒæ•°æ®çš„è¿‡æ»¤,å¹¶å°†åœ¨åå°èšåˆå®Œæ•´æ¶ˆæ¯ã€‚è¯·å‚é˜…ä¸‹é¢çš„ç¤ºä¾‹ã€‚

```python
async for event in model.astream_events("ä½ å¥½"):

    if event["event"] == "on_chat_model_start":
        print(f"Input: {event['data']['input']}")

    elif event["event"] == "on_chat_model_stream":
        print(f"Token: {event['data']['chunk'].content}")

    elif event["event"] == "on_chat_model_end":
        print(f"Full message: {event['data']['output'].content}")

    else:
        pass
```

è¾“å‡ºç¤ºä¾‹:

```txt
Input: ä½ å¥½
Token: ä½ 
Token: å¥½
Token: ï¼
Token: æœ‰
Token: ä»€
Token: ä¹ˆ
...
Full message: ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ
```

> **ğŸ’¡ æç¤º**: æœ‰å…³äº‹ä»¶ç±»å‹å’Œå…¶ä»–è¯¦ç»†ä¿¡æ¯,è¯·å‚é˜… [`astream_events()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.astream_events) å‚è€ƒæ–‡æ¡£ã€‚

### Batch

Batching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:

```python
responses = model.batch([
    "ä¸ºä»€ä¹ˆé¹¦é¹‰æœ‰äº”é¢œå…­è‰²çš„ç¾½æ¯›ï¼Ÿ",
    "é£æœºæ˜¯å¦‚ä½•é£è¡Œçš„ï¼Ÿ",
    "ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—ï¼Ÿ"
])
for response in responses:
    print(response.content)
```

> **ğŸ“ æ³¨æ„**: æœ¬èŠ‚æè¿°çš„æ˜¯èŠå¤©æ¨¡å‹æ–¹æ³• [`batch()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch),å®ƒåœ¨å®¢æˆ·ç«¯å¹¶è¡ŒåŒ–æ¨¡å‹è°ƒç”¨ã€‚è¿™ä¸æ¨ç†æä¾›å•†æ”¯æŒçš„æ‰¹å¤„ç† API **ä¸åŒ**,ä¾‹å¦‚ [OpenAI](https://platform.openai.com/docs/guides/batch) æˆ– [Anthropic](https://docs.claude.com/en/docs/build-with-claude/batch-processing#message-batches-api)ã€‚

By default, [`batch()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch) will only return the final output for the entire batch. If you want to receive the output for each individual input as it finishes generating, you can stream results with [`batch_as_completed()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch_as_completed):

```python
for response in model.batch_as_completed([
    "ä¸ºä»€ä¹ˆé¹¦é¹‰æœ‰äº”é¢œå…­è‰²çš„ç¾½æ¯›ï¼Ÿ",
    "é£æœºæ˜¯å¦‚ä½•é£è¡Œçš„ï¼Ÿ",
    "ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—ï¼Ÿ"
]):
    print(response)
```

> **ğŸ“ æ³¨æ„**: ä½¿ç”¨ [`batch_as_completed()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch_as_completed) æ—¶,ç»“æœå¯èƒ½ä¼šä¹±åºåˆ°è¾¾ã€‚æ¯ä¸ªç»“æœéƒ½åŒ…å«è¾“å…¥ç´¢å¼•,ä»¥ä¾¿åœ¨éœ€è¦æ—¶åŒ¹é…å¹¶é‡å»ºåŸå§‹é¡ºåºã€‚

> **ğŸ’¡ æç¤º**: ä½¿ç”¨ [`batch()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch) æˆ– [`batch_as_completed()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch_as_completed) å¤„ç†å¤§é‡è¾“å…¥æ—¶,æ‚¨å¯èƒ½å¸Œæœ›æ§åˆ¶å¹¶è¡Œè°ƒç”¨çš„æœ€å¤§æ•°é‡ã€‚å¯ä»¥é€šè¿‡åœ¨ [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) å­—å…¸ä¸­è®¾ç½® [`max_concurrency`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig.max_concurrency) å±æ€§æ¥å®ç°ã€‚
>
> ```python
> model.batch(
>     list_of_inputs,
>     config={
>         'max_concurrency': 5,  # Limit to 5 parallel calls
>     }
> )
> ```
>
> æœ‰å…³æ”¯æŒçš„æ‰€æœ‰å±æ€§çš„å®Œæ•´åˆ—è¡¨,è¯·å‚é˜… [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) å‚è€ƒæ–‡æ¡£ã€‚

## ç»“æ„åŒ–è¾“å‡º (Structured outputs)

å¯ä»¥è¯·æ±‚æ¨¡å‹ä»¥ç¬¦åˆç»™å®šæ¨¡å¼çš„æ ¼å¼æä¾›å“åº”ã€‚è¿™å¯¹äºç¡®ä¿è¾“å‡ºå¯ä»¥è½»æ¾è§£æå¹¶ç”¨äºåç»­å¤„ç†éå¸¸æœ‰ç”¨ã€‚LangChain æ”¯æŒå¤šç§æ¨¡å¼ç±»å‹å’Œå¼ºåˆ¶æ‰§è¡Œç»“æ„åŒ–è¾“å‡ºçš„æ–¹æ³•ã€‚

**è¯¦ç»†è¯´æ˜ï¼š** ç»“æ„åŒ–è¾“å‡ºè§£å†³äº† LLM è¾“å‡ºè§£æçš„éš¾é¢˜ã€‚æ²¡æœ‰ç»“æ„åŒ–è¾“å‡ºæ—¶ï¼Œæ‚¨éœ€è¦ï¼š

1. åœ¨æç¤ºä¸­æè¿°æœŸæœ›çš„æ ¼å¼ï¼ˆå¯èƒ½ä¸å¯é ï¼‰
2. ç¼–å†™å¤æ‚çš„è§£æé€»è¾‘æ¥æå–ä¿¡æ¯
3. å¤„ç†å„ç§æ ¼å¼é”™è¯¯å’Œè¾¹ç¼˜æƒ…å†µ
4. éªŒè¯æå–çš„æ•°æ®

ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºï¼š

- **ä¿è¯æ ¼å¼æ­£ç¡®**: æ¨¡å‹çš„è¾“å‡ºè‡ªåŠ¨ç¬¦åˆå®šä¹‰çš„æ¨¡å¼
- **ç±»å‹å®‰å…¨**: ä½¿ç”¨ Python ç±»å‹æç¤ºï¼Œè·å¾— IDE è‡ªåŠ¨å®Œæˆå’Œç±»å‹æ£€æŸ¥
- **è‡ªåŠ¨éªŒè¯**: Pydantic ç­‰å·¥å…·è‡ªåŠ¨éªŒè¯æ•°æ®
- **æ˜“äºé›†æˆ**: è¾“å‡ºå¯ä»¥ç›´æ¥ç”¨ä½œå‡½æ•°å‚æ•°æˆ–ä¿å­˜åˆ°æ•°æ®åº“

å®é™…åº”ç”¨åœºæ™¯ï¼š

- **æ•°æ®æå–**: ä»æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼ˆå§“åã€æ—¥æœŸã€åœ°å€ç­‰ï¼‰
- **åˆ†ç±»**: å°†æ–‡æœ¬åˆ†ç±»åˆ°é¢„å®šä¹‰ç±»åˆ«
- **è¡¨å•å¡«å……**: ä»è‡ªç„¶è¯­è¨€ç”Ÿæˆè¡¨å•æ•°æ®
- **API é›†æˆ**: ç”Ÿæˆç¬¦åˆ API è¦æ±‚çš„è¯·æ±‚æ•°æ®
- **æ•°æ®è½¬æ¢**: å°†ä¸€ç§æ ¼å¼çš„æ•°æ®è½¬æ¢ä¸ºå¦ä¸€ç§æ ¼å¼

#### ä½¿ç”¨ Pydantic

[Pydantic models](https://docs.pydantic.dev/latest/concepts/models/#basic-model-usage) æä¾›äº†æœ€ä¸°å¯Œçš„åŠŸèƒ½é›†,åŒ…æ‹¬å­—æ®µéªŒè¯ã€æè¿°å’ŒåµŒå¥—ç»“æ„ã€‚

```python
from pydantic import BaseModel, Field

class Movie(BaseModel):
    """A movie with details."""
    title: str = Field(..., description="The title of the movie")
    year: int = Field(..., description="The year the movie was released")
    director: str = Field(..., description="The director of the movie")
    rating: float = Field(..., description="The movie's rating out of 10")

model_with_structure = model.with_structured_output(Movie)
response = model_with_structure.invoke("æä¾›ç”µå½±ã€Šç›—æ¢¦ç©ºé—´ã€‹çš„è¯¦ç»†ä¿¡æ¯")
print(response)  # Movie(title="ç›—æ¢¦ç©ºé—´", year=2010, director="å…‹é‡Œæ–¯æ‰˜å¼—Â·è¯ºå…°", rating=8.8)
```

#### ä½¿ç”¨ JSON Schema

ä¸ºäº†è·å¾—æœ€å¤§çš„æ§åˆ¶æˆ–äº’æ“ä½œæ€§,æ‚¨å¯ä»¥æä¾›åŸå§‹ JSON Schemaã€‚

```python
import json

json_schema = {
    "title": "Movie",
    "description": "A movie with details",
    "type": "object",
    "properties": {
        "title": {
            "type": "string",
            "description": "The title of the movie"
        },
        "year": {
            "type": "integer",
            "description": "The year the movie was released"
        },
        "director": {
            "type": "string",
            "description": "The director of the movie"
        },
        "rating": {
            "type": "number",
            "description": "The movie's rating out of 10"
        }
    },
    "required": ["title", "year", "director", "rating"]
}

model_with_structure = model.with_structured_output(
    json_schema,
    method="json_schema",
)
response = model_with_structure.invoke("æä¾›ç”µå½±ã€Šç›—æ¢¦ç©ºé—´ã€‹çš„è¯¦ç»†ä¿¡æ¯")
print(response)  # {'title': 'ç›—æ¢¦ç©ºé—´', 'year': 2010, ...}
```

> **ğŸ“ æ³¨æ„**:
>
> **ç»“æ„åŒ–è¾“å‡ºçš„å…³é”®è€ƒè™‘å› ç´ :**
>
> - **Method å‚æ•°**: ä¸€äº›æä¾›å•†æ”¯æŒä¸åŒçš„æ–¹æ³• (`'json_schema'`, `'function_calling'`, `'json_mode'`)
>   - `'json_schema'` é€šå¸¸æ˜¯æŒ‡æä¾›å•†æä¾›çš„ä¸“ç”¨ç»“æ„åŒ–è¾“å‡ºåŠŸèƒ½
>   - `'function_calling'` é€šè¿‡å¼ºåˆ¶éµå¾ªç»™å®šæ¨¡å¼çš„[å·¥å…·è°ƒç”¨](#tool-calling)æ¥æ´¾ç”Ÿç»“æ„åŒ–è¾“å‡º
>   - `'json_mode'` æ˜¯ä¸€äº›æä¾›å•†æä¾›çš„ `'json_schema'` çš„å‰èº« - å®ƒç”Ÿæˆæœ‰æ•ˆçš„ json,ä½†å¿…é¡»åœ¨æç¤ºä¸­æè¿°æ¨¡å¼
> - **Include raw**: ä½¿ç”¨ `include_raw=True` å¯ä»¥åŒæ—¶è·å–è§£æçš„è¾“å‡ºå’ŒåŸå§‹ AI æ¶ˆæ¯
> - **Validation**: Pydantic æ¨¡å‹æä¾›è‡ªåŠ¨éªŒè¯,è€Œ `TypedDict` å’Œ JSON Schema éœ€è¦æ‰‹åŠ¨éªŒè¯

#### ç¤ºä¾‹:è§£æç»“æ„å’Œæ¶ˆæ¯è¾“å‡º

It can be useful to return the raw [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) object alongside the parsed representation to access response metadata such as [token counts](#token-usage). To do this, set [`include_raw=True`](<https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.with_structured_output(include_raw)>) when calling [`with_structured_output`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.with_structured_output):

```python theme={null}
from pydantic import BaseModel, Field

class Movie(BaseModel):
    """A movie with details."""
    title: str = Field(..., description="The title of the movie")
    year: int = Field(..., description="The year the movie was released")
    director: str = Field(..., description="The director of the movie")
    rating: float = Field(..., description="The movie's rating out of 10")

model_with_structure = model.with_structured_output(Movie, include_raw=True)  # [!code highlight]
response = model_with_structure.invoke("æä¾›ç”µå½±ã€Šç›—æ¢¦ç©ºé—´ã€‹çš„è¯¦ç»†ä¿¡æ¯")
response
# {
#     "raw": AIMessage(...),
#     "parsed": Movie(title=..., year=..., ...),
#     "parsing_error": None,
# }
```

## é«˜çº§ä¸»é¢˜ (Advanced topics)

### å¤šæ¨¡æ€ (Multimodal)

æŸäº›æ¨¡å‹å¯ä»¥å¤„ç†å’Œè¿”å›éæ–‡æœ¬æ•°æ®ï¼Œå¦‚å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ã€‚æ‚¨å¯ä»¥é€šè¿‡æä¾›[å†…å®¹å—](/oss/python/langchain/messages#message-content)å‘æ¨¡å‹ä¼ é€’éæ–‡æœ¬æ•°æ®ã€‚

**è¯¦ç»†è¯´æ˜ï¼š** å¤šæ¨¡æ€èƒ½åŠ›ä½¿ AI åº”ç”¨èƒ½å¤Ÿå¤„ç†çœŸå®ä¸–ç•Œçš„ä¸°å¯Œæ•°æ®ã€‚è¿™å¼€å¯äº†è®¸å¤šæ–°çš„åº”ç”¨åœºæ™¯ï¼š

**è§†è§‰ç†è§£**:

- å›¾åƒåˆ†æå’Œæè¿°
- æ–‡æ¡£ç†è§£ï¼ˆæ‰«æä»¶ã€å‘ç¥¨ã€è¡¨æ ¼ï¼‰
- å›¾è¡¨å’Œå›¾å½¢è§£è¯»
- OCRï¼ˆå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼‰
- è§†è§‰é—®ç­”

**éŸ³é¢‘å¤„ç†**:

- è¯­éŸ³è½¬æ–‡æœ¬
- éŸ³é¢‘å†…å®¹ç†è§£
- å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«

**å¤šæ¨¡æ€è¾“å‡º**:

- å›¾åƒç”Ÿæˆï¼ˆæ–‡æœ¬åˆ°å›¾åƒï¼‰
- å›¾åƒç¼–è¾‘å’Œä¿®æ”¹
- å›¾è¡¨å’Œå¯è§†åŒ–åˆ›å»º

å®é™…åº”ç”¨ç¤ºä¾‹ï¼š

- æ™ºèƒ½æ–‡æ¡£å¤„ç†ç³»ç»Ÿï¼ˆç†è§£æ‰«æçš„åˆåŒã€å‘ç¥¨ï¼‰
- åŒ»å­¦å½±åƒåˆ†æåŠ©æ‰‹
- æ•™è‚²åº”ç”¨ï¼ˆè§£é‡Šæ•™ç§‘ä¹¦å›¾è¡¨ï¼‰
- å®¢æˆ·æœåŠ¡æœºå™¨äººï¼ˆç†è§£ç”¨æˆ·ä¸Šä¼ çš„æˆªå›¾ï¼‰
- å†…å®¹å®¡æ ¸ç³»ç»Ÿ

> **ğŸ’¡ æç¤º**: æ‰€æœ‰å…·æœ‰åº•å±‚å¤šæ¨¡æ€åŠŸèƒ½çš„ LangChain èŠå¤©æ¨¡å‹éƒ½æ”¯æŒ:
>
> 1. è·¨æä¾›å•†æ ‡å‡†æ ¼å¼çš„æ•°æ®ï¼ˆå‚è§[æˆ‘ä»¬çš„æ¶ˆæ¯æŒ‡å—](/oss/python/langchain/messages)ï¼‰
> 2. OpenAI [èŠå¤©å®Œæˆ](https://platform.openai.com/docs/api-reference/chat)æ ¼å¼
> 3. ç‰¹å®šæä¾›å•†åŸç”Ÿçš„ä»»ä½•æ ¼å¼ï¼ˆä¾‹å¦‚ï¼ŒAnthropic æ¨¡å‹æ¥å— Anthropic åŸç”Ÿæ ¼å¼ï¼‰

See the [multimodal section](/oss/python/langchain/messages#multimodal) of the messages guide for details.

[æŸäº›æ¨¡å‹](https://models.dev/)å¯ä»¥å°†å¤šæ¨¡æ€æ•°æ®ä½œä¸ºå…¶å“åº”çš„ä¸€éƒ¨åˆ†è¿”å›ï¼ˆæ³¨æ„:å¹¶éæ‰€æœ‰ LLM éƒ½æ˜¯å¹³ç­‰çš„!ï¼‰ã€‚å¦‚æœè¢«è°ƒç”¨è¿™æ ·åš,ç”Ÿæˆçš„ [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) å°†å…·æœ‰å¤šæ¨¡æ€ç±»å‹çš„å†…å®¹å—ã€‚

```python
response = model.invoke("åˆ›å»ºä¸€å¼ çŒ«çš„å›¾ç‰‡")
print(response.content)
# æ³¨æ„ï¼šqwen-plus ä¸æ”¯æŒå›¾åƒç”Ÿæˆï¼Œè¿™é‡Œä»…ä½œç¤ºä¾‹
# æ”¯æŒå¤šæ¨¡æ€çš„æ¨¡å‹ä¼šè¿”å›åŒ…å«å›¾åƒæ•°æ®çš„å“åº”
```

See the [integrations page](/oss/python/integrations/providers/overview) for details on specific providers.

### æ¨ç† (Reasoning)

è¾ƒæ–°çš„æ¨¡å‹èƒ½å¤Ÿæ‰§è¡Œå¤šæ­¥æ¨ç†ä»¥å¾—å‡ºç»“è®ºã€‚è¿™æ¶‰åŠå°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºæ›´å°ã€æ›´æ˜“ç®¡ç†çš„æ­¥éª¤ã€‚

**è¯¦ç»†è¯´æ˜ï¼š** æ¨ç†èƒ½åŠ›ä»£è¡¨äº† LLM çš„é‡å¤§è¿›æ­¥ã€‚ä¼ ç»Ÿæ¨¡å‹å€¾å‘äºç›´æ¥ç»™å‡ºç­”æ¡ˆï¼Œè€Œæ¨ç†æ¨¡å‹ä¼šï¼š

**å·¥ä½œæ–¹å¼**:

1. **åˆ†æé—®é¢˜**: ç†è§£é—®é¢˜çš„å„ä¸ªç»„æˆéƒ¨åˆ†
2. **åˆ¶å®šè®¡åˆ’**: ç¡®å®šè§£å†³é—®é¢˜çš„æ­¥éª¤
3. **é€æ­¥æ±‚è§£**: æ‰§è¡Œæ¯ä¸ªæ­¥éª¤ï¼Œä½¿ç”¨ä¸­é—´ç»“æœ
4. **éªŒè¯ç­”æ¡ˆ**: æ£€æŸ¥ç»“æœæ˜¯å¦åˆç†
5. **æä¾›æœ€ç»ˆç­”æ¡ˆ**: ç»™å‡ºç»è¿‡éªŒè¯çš„ç»“è®º

**ä¼˜åŠ¿**:

- **æ›´é«˜å‡†ç¡®æ€§**: ç‰¹åˆ«æ˜¯å¯¹äºéœ€è¦å¤šæ­¥é€»è¾‘çš„é—®é¢˜
- **å¯è§£é‡Šæ€§**: æ‚¨å¯ä»¥çœ‹åˆ°æ¨¡å‹çš„"æ€è€ƒè¿‡ç¨‹"
- **é”™è¯¯è¯Šæ–­**: å½“ç­”æ¡ˆé”™è¯¯æ—¶ï¼Œå¯ä»¥å®šä½é—®é¢˜å‡ºåœ¨å“ªä¸€æ­¥
- **å¯é æ€§**: å‡å°‘"å¹»è§‰"ï¼ˆç¼–é€ ä¿¡æ¯ï¼‰çš„æƒ…å†µ

**é€‚ç”¨åœºæ™¯**:

- æ•°å­¦é—®é¢˜æ±‚è§£
- é€»è¾‘æ¨ç†å’Œè°œé¢˜
- ä»£ç è°ƒè¯•å’Œé”™è¯¯åˆ†æ
- å¤æ‚å†³ç­–åˆ¶å®š
- ç§‘å­¦é—®é¢˜åˆ†æ
- æ³•å¾‹æ¡ˆä¾‹åˆ†æ

**å¦‚æœåº•å±‚æ¨¡å‹æ”¯æŒï¼Œ** æ‚¨å¯ä»¥å±•ç¤ºè¿™ä¸ªæ¨ç†è¿‡ç¨‹ï¼Œä»¥æ›´å¥½åœ°ç†è§£æ¨¡å‹å¦‚ä½•å¾—å‡ºæœ€ç»ˆç­”æ¡ˆã€‚

**è¯¦ç»†è¯´æ˜ï¼š** è®¿é—®æ¨ç†è¿‡ç¨‹ä¸ä»…æœ‰åŠ©äºè°ƒè¯•å’ŒéªŒè¯ï¼Œè¿˜èƒ½ç”¨äºæ•™è‚²åœºæ™¯ï¼ˆå±•ç¤ºè§£é¢˜æ­¥éª¤ï¼‰æˆ–å¢å¼ºç”¨æˆ·å¯¹ AI å†³ç­–çš„ä¿¡ä»»ã€‚

**æµå¼è¾“å‡ºæ¨ç†è¿‡ç¨‹:**

```python
for chunk in model.stream("ä¸ºä»€ä¹ˆé¹¦é¹‰æœ‰äº”é¢œå…­è‰²çš„ç¾½æ¯›ï¼Ÿ"):
    print(chunk.content, end="", flush=True)
```

**å®Œæ•´æ¨ç†è¾“å‡º:**

```python
response = model.invoke("ä¸ºä»€ä¹ˆé¹¦é¹‰æœ‰äº”é¢œå…­è‰²çš„ç¾½æ¯›ï¼Ÿ")
print(response.content)
```

Depending on the model, you can sometimes specify the level of effort it should put into reasoning. Similarly, you can request that the model turn off reasoning entirely. This may take the form of categorical "tiers" of reasoning (e.g., `'low'` or `'high'`) or integer token budgets.

For details, see the [integrations page](/oss/python/integrations/providers/overview) or [reference](https://reference.langchain.com/python/integrations/) for your respective chat model.

### æç¤ºç¼“å­˜ (Prompt caching)

è®¸å¤šæä¾›å•†æä¾›æç¤ºç¼“å­˜åŠŸèƒ½ï¼Œä»¥å‡å°‘é‡å¤å¤„ç†ç›¸åŒ token æ—¶çš„å»¶è¿Ÿå’Œæˆæœ¬ã€‚è¿™äº›åŠŸèƒ½å¯ä»¥æ˜¯**éšå¼**æˆ–**æ˜¾å¼**çš„ï¼š

**è¯¦ç»†è¯´æ˜ï¼š** æç¤ºç¼“å­˜æ˜¯ä¸€ç§æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯ï¼Œå…¶å·¥ä½œåŸç†ï¼š

**å·¥ä½œæœºåˆ¶**:

1. æä¾›å•†å­˜å‚¨æœ€è¿‘å¤„ç†è¿‡çš„æç¤ºï¼ˆæˆ–æç¤ºçš„ä¸€éƒ¨åˆ†ï¼‰
2. å½“æ”¶åˆ°æ–°è¯·æ±‚æ—¶ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰åŒ¹é…çš„ç¼“å­˜å†…å®¹
3. å¦‚æœå‘½ä¸­ç¼“å­˜ï¼Œè·³è¿‡é‡æ–°å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜ç»“æœ
4. åªå¤„ç†ç¼“å­˜åçš„æ–°å†…å®¹

**ä¼˜åŠ¿**:

- **é™ä½å»¶è¿Ÿ**: ç¼“å­˜å‘½ä¸­æ—¶å“åº”é€Ÿåº¦å¯æå‡ 80-90%
- **å‡å°‘æˆæœ¬**: ç¼“å­˜çš„ token é€šå¸¸æŒ‰æ›´ä½çš„ä»·æ ¼è®¡è´¹ï¼ˆæˆ–å…è´¹ï¼‰
- **æé«˜ååé‡**: æœåŠ¡å™¨èµ„æºå¾—åˆ°æ›´æœ‰æ•ˆåˆ©ç”¨

**é€‚ç”¨åœºæ™¯**:

- å…·æœ‰å›ºå®šç³»ç»Ÿæç¤ºçš„å¯¹è¯åº”ç”¨
- éœ€è¦é‡å¤åˆ†æç›¸åŒæ–‡æ¡£çš„åœºæ™¯
- ä½¿ç”¨å¤§é‡ç¤ºä¾‹çš„ few-shot å­¦ä¹ 
- RAG ç³»ç»Ÿï¼ˆæ£€ç´¢ç›¸åŒçš„ä¸Šä¸‹æ–‡æ–‡æ¡£ï¼‰

**æ³¨æ„äº‹é¡¹**:

- ç¼“å­˜é€šå¸¸æœ‰æ—¶é—´é™åˆ¶ï¼ˆå¦‚ 5-15 åˆ†é’Ÿï¼‰
- æç¤ºå¿…é¡»å®Œå…¨ç›¸åŒæ‰èƒ½å‘½ä¸­ç¼“å­˜
- æŸäº›æä¾›å•†è¦æ±‚æœ€å° token æ•°é‡æ‰å¯ç”¨ç¼“å­˜

- **Implicit prompt caching:** providers will automatically pass on cost savings if a request hits a cache. Examples: [OpenAI](/oss/python/integrations/chat/openai) and [Gemini](/oss/python/integrations/chat/google_generative_ai) (Gemini 2.5 and above).
- **Explicit caching:** providers allow you to manually indicate cache points for greater control or to guarantee cost savings. Examples: [`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain_openai/ChatOpenAI/) (via `prompt_cache_key`), Anthropic's [`AnthropicPromptCachingMiddleware`](/oss/python/integrations/chat/anthropic#prompt-caching) and [`cache_control`](https://docs.langchain.com/oss/python/integrations/chat/anthropic#prompt-caching) options, [AWS Bedrock](/oss/python/integrations/chat/bedrock#prompt-caching), [Gemini](https://python.langchain.com/api_reference/google_genai/chat_models/langchain_google_genai.chat_models.ChatGoogleGenerativeAI.html).

> **âš ï¸ è­¦å‘Š**: æç¤ºç¼“å­˜é€šå¸¸ä»…åœ¨è¶…è¿‡æœ€å°è¾“å…¥ token é˜ˆå€¼æ—¶æ‰å¯ç”¨ã€‚è¯¦è§[æä¾›å•†é¡µé¢](/oss/python/integrations/chat)ã€‚

Cache usage will be reflected in the [usage metadata](/oss/python/langchain/messages#token-usage) of the model response.

### Token ä½¿ç”¨é‡ (Token usage)

è®¸å¤šæ¨¡å‹æä¾›å•†å°† token ä½¿ç”¨ä¿¡æ¯ä½œä¸ºè°ƒç”¨å“åº”çš„ä¸€éƒ¨åˆ†è¿”å›ã€‚å½“å¯ç”¨æ—¶ï¼Œæ­¤ä¿¡æ¯å°†åŒ…å«åœ¨ç›¸åº”æ¨¡å‹ç”Ÿæˆçš„ [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) å¯¹è±¡ä¸­ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[æ¶ˆæ¯æŒ‡å—](/oss/python/langchain/messages)ã€‚

**è¯¦ç»†è¯´æ˜ï¼š** Token ä½¿ç”¨é‡è·Ÿè¸ªå¯¹äºä»¥ä¸‹æ–¹é¢è‡³å…³é‡è¦ï¼š

**ä¸ºä»€ä¹ˆè·Ÿè¸ª Token ä½¿ç”¨é‡**:

- **æˆæœ¬ç®¡ç†**: å¤§å¤šæ•°æä¾›å•†æŒ‰ token è®¡è´¹ï¼Œè·Ÿè¸ªä½¿ç”¨é‡å¯æ§åˆ¶æˆæœ¬
- **æ€§èƒ½ä¼˜åŒ–**: è¯†åˆ«å¯ä»¥ä¼˜åŒ–æç¤ºä»¥å‡å°‘ token çš„åœ°æ–¹
- **é…é¢ç®¡ç†**: ç›‘æ§æ˜¯å¦æ¥è¿‘é€Ÿç‡é™åˆ¶
- **åº”ç”¨åˆ†æ**: äº†è§£ä¸åŒåŠŸèƒ½çš„èµ„æºæ¶ˆè€—

**Token ç±»å‹**:

- **è¾“å…¥ Token (Input tokens)**: æ‚¨å‘é€ç»™æ¨¡å‹çš„æ–‡æœ¬ï¼ˆæç¤ºã€æ¶ˆæ¯å†å²ã€æ–‡æ¡£ç­‰ï¼‰
- **è¾“å‡º Token (Output tokens)**: æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬
- **ç¼“å­˜ Token (Cached tokens)**: ä»ç¼“å­˜ä¸­è¯»å–çš„ tokenï¼ˆå¦‚æœå¯ç”¨ç¼“å­˜ï¼‰
- **æ¨ç† Token (Reasoning tokens)**: æŸäº›æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨çš„å†…éƒ¨ token

**æˆæœ¬è€ƒè™‘**:

- è¾“å‡º token é€šå¸¸æ¯”è¾“å…¥ token æ›´è´µï¼ˆå¯èƒ½æ˜¯ 2-3 å€ï¼‰
- ç¼“å­˜å‘½ä¸­çš„ token é€šå¸¸å…è´¹æˆ–å¤§å¹…æŠ˜æ‰£
- ä¸åŒæ¨¡å‹çš„ token ä»·æ ¼å·®å¼‚å¾ˆå¤§
- æ‰¹å¤„ç†è°ƒç”¨å¯èƒ½æœ‰æŠ˜æ‰£

> **ğŸ“ æ³¨æ„**: æŸäº›æä¾›å•† APIï¼Œç‰¹åˆ«æ˜¯ OpenAI å’Œ Azure OpenAI èŠå¤©è¡¥å…¨ï¼Œè¦æ±‚ç”¨æˆ·é€‰æ‹©æ¥æ”¶æµå¼ä¸Šä¸‹æ–‡ä¸­çš„ token ä½¿ç”¨æ•°æ®ã€‚è¯¦è§é›†æˆæŒ‡å—çš„[æµå¼ä½¿ç”¨å…ƒæ•°æ®](/oss/python/integrations/chat/openai#streaming-usage-metadata)éƒ¨åˆ†ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨å›è°ƒæˆ–ä¸Šä¸‹æ–‡ç®¡ç†å™¨è·Ÿè¸ªåº”ç”¨ç¨‹åºä¸­è·¨æ¨¡å‹çš„èšåˆ token è®¡æ•°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

**è¯¦ç»†è¯´æ˜ï¼š** LangChain æä¾›äº†ä¸¤ç§è·Ÿè¸ªæ–¹å¼ï¼š

- **å›è°ƒå¤„ç†å™¨**: é€‚åˆéœ€è¦åœ¨æ•´ä¸ªåº”ç”¨ç”Ÿå‘½å‘¨æœŸä¸­è·Ÿè¸ªçš„åœºæ™¯
- **ä¸Šä¸‹æ–‡ç®¡ç†å™¨**: é€‚åˆè·Ÿè¸ªç‰¹å®šä»£ç å—çš„ä½¿ç”¨é‡ï¼Œè‡ªåŠ¨æ¸…ç†èµ„æº

#### ä½¿ç”¨å›è°ƒå¤„ç†å™¨

```python
from langchain.chat_models import init_chat_model
from langchain_core.callbacks import UsageMetadataCallbackHandler
import os
from dotenv import load_dotenv

load_dotenv()

model = init_chat_model(
    model="qwen-plus",
    model_provider="openai",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    api_key=os.getenv("DASHSCOPE_API_KEY"),
)

callback = UsageMetadataCallbackHandler()
result = model.invoke("ä½ å¥½", config={"callbacks": [callback]})
print(callback.usage_metadata)
```

è¾“å‡ºç¤ºä¾‹:

```python
{
    'qwen-plus': {
        'input_tokens': 2,
        'output_tokens': 15,
        'total_tokens': 17
    }
}
```

#### ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨

```python
from langchain.chat_models import init_chat_model
from langchain_core.callbacks import get_usage_metadata_callback
import os
from dotenv import load_dotenv

load_dotenv()

model = init_chat_model(
    model="qwen-plus",
    model_provider="openai",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    api_key=os.getenv("DASHSCOPE_API_KEY"),
)

with get_usage_metadata_callback() as cb:
    model.invoke("ä½ å¥½")
    model.invoke("å†è§")
    print(cb.usage_metadata)
```

è¾“å‡ºç¤ºä¾‹:

```python
{
    'qwen-plus': {
        'input_tokens': 4,
        'output_tokens': 30,
        'total_tokens': 34
    }
}
```
